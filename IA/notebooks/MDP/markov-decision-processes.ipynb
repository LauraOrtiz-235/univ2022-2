{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpmFfXsQ0dYI"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp>\n",
    "                <h1 style=\"color:blue;text-align:center\">Inteligencia Artificial</h1\n",
    "            </tp>\n",
    "            <tp>\n",
    "                <p style=\"font-size:150%;text-align:center\">Markov Decision Processes</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "# Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook veremos una manera de implementar los ambientes de tarea de los MDP. Implementaremos manualmente algunos MDP.\n",
    "\n",
    "Este notebook está basado en las presentación de Sanghi (2021), capítulo 2 y sus [notebooks](https://github.com/Apress/deep-reinforcement-learning-python); Sutton R., & Barto, A., (2015), capítulos 3 y 4; y también Winder, P., (2021), capítulo 2 y su [notebook](https://rl-book.com/learn/mdp/code_driven_intro/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "# Secciones\n",
    "\n",
    "Desarrollaremos la explicación de la siguiente manera:\n",
    "\n",
    "1. [Ejemplos de implementación](#impl).\n",
    "    1. [GridWorld](#gw).\n",
    "    2. [Tienda](#tienda).\n",
    "2. [Evaluación de políticas](#poli-eval).\n",
    "3. [Mejoramiento de políticas](#dp).\n",
    "    1. [Policy iteration](#poli-iter).\n",
    "    2. [Value iteration](#value-iter).\n",
    "    3. [Comparación de tiempos](#comp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias\n",
    "\n",
    "Las librerías que usaremos en este notebook son las siguientes. Por favor corra esta celda siempre que inicie el notebook o cuando reinicie el Kernel, instalando las librerías que hagan falta en su ambiente de python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage, TextArea\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from typing import Tuple\n",
    "from Tiempos import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4x5lJfd0dYM"
   },
   "source": [
    "# Ejemplos de implementación <a class=\"anchor\" id=\"impl\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Comenzaremos con la implementación de un ejemplo muy usado para visualizar valores de estados y políticas, que es el ejemplo del Grid World. También tendremos un ejemplo más sencillo, que se le pedirá a usted terminar, que es el de la tienda con un solo artículo, el cual discutimos en las diapositivas de clase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World <a class=\"anchor\" id=\"gw\"></a>\n",
    "\n",
    "([Volver a Ejemplos](#impl))\n",
    "\n",
    "Un ejemplo muy útil en términos de visualización de MDP es el del Grid World, el cual consiste de una rejilla rectangular. Las casillas corresponden a los estados. Hay cuatro acciones: norte, sur, este y oeste, que hacen que el agente se mueva una casilla en la dirección respectiva en la rejilla. Las acciones que sacarían al agente de la rejilla dejan su ubicación sin cambios. Cada acción da como resultado una recompensa de -1. Las casillas (0,0) y (4,4) son estados terminales.\n",
    "\n",
    "Veamos una implementación *ad hoc* tomada del libro de Sanghi (ver [código](https://github.com/Apress/deep-reinforcement-learning-python/blob/main/chapter3/gridworld.py)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the actions\n",
    "NORTH = 0\n",
    "EAST = 1\n",
    "SOUTH = 2\n",
    "WEST = 3\n",
    "dict_acciones = {0:\"⬆\", 1:\"➡\", 2:\"⬇\", 3:\"⬅\"}\n",
    "\n",
    "class GridworldEnv():\n",
    "    \"\"\"\n",
    "    A 4x4 Grid World environment from Sutton's Reinforcement \n",
    "    Learning book chapter 4. Termial states are top left and\n",
    "    the bottom right corner.\n",
    "    Actions are (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "    Actions going off the edge leave agent in current state.\n",
    "    Reward of -1 at each step until agent reachs a terminal state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape=(4,4)):\n",
    "        self.shape = shape\n",
    "        self.nS = np.prod(self.shape)\n",
    "        self.nA = 4\n",
    "        self.action_space = list(range(self.nA))\n",
    "        self.state = 2\n",
    "        P = {}\n",
    "        for s in range(self.nS):\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            # Per state and action provide list as follows\n",
    "            # P[state][action] = [(probability, next_state, reward, done)]\n",
    "            # Assignment is obtained by means of method _transition_prob\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s][NORTH] = self._transition_prob(position, [-1, 0])\n",
    "            P[s][EAST] = self._transition_prob(position, [0, 1])\n",
    "            P[s][SOUTH] = self._transition_prob(position, [1, 0])\n",
    "            P[s][WEST] = self._transition_prob(position, [0, -1])\n",
    "        # We expose the model of the environment for dynamic programming\n",
    "        # This should not be used in any model-free learning algorithm\n",
    "        self.P = P\n",
    "\n",
    "    def _limit_coordinates(self, coord):\n",
    "        \"\"\"\n",
    "        Prevent the agent from falling out of the grid world\n",
    "        :param coord:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "    def _transition_prob(self, current, delta):\n",
    "        \"\"\"\n",
    "        Model Transitions. Prob is always 1.0.\n",
    "        :param current: Current position on the grid as (row, col)\n",
    "        :param delta: Change in position for transition\n",
    "        :return: [(1.0, new_state, reward, done)]\n",
    "        \"\"\"\n",
    "        # if stuck in terminal state\n",
    "        current_state = np.ravel_multi_index(tuple(current), self.shape)\n",
    "        if current_state == 0 or current_state == self.nS - 1:\n",
    "            return [(1.0, current_state, 0, True)]\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        is_done = new_state == 0 or new_state == self.nS - 1\n",
    "        return [(1.0, new_state, -1, is_done)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = random.randint(1, self.nS - 2)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        s = self.state\n",
    "        p = self.P[s][action]\n",
    "        indice = random.choices(\n",
    "            population = range(len(p)),\n",
    "            weights = [x[0] for x in p],\n",
    "            k = 1\n",
    "        )[0]\n",
    "        new_state = p[indice][1]\n",
    "        self.state = new_state\n",
    "        reward = p[indice][2]\n",
    "        done = p[indice][3]\n",
    "        return new_state, reward, done    \n",
    "\n",
    "    def render(self):\n",
    "        state = self.state\n",
    "        output = ''\n",
    "        for s in range(self.nS):\n",
    "            if s == state:\n",
    "                if state == 0 or state == self.nS - 1:\n",
    "                    output += '@'\n",
    "                else:\n",
    "                    output += \"x\"\n",
    "            # Print terminal state\n",
    "            elif s == 0 or s == self.nS - 1:\n",
    "                output += \"o\"\n",
    "            else:\n",
    "                output += \"_\"\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += '\\n'\n",
    "        print(output)\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for s in range(self.nS):\n",
    "            string += '\\n'+'-'*20\n",
    "            string += f'\\nState:{np.unravel_index(s, self.shape)}'\n",
    "            for a in range(self.nA):\n",
    "                string += f'\\nAction:{dict_acciones[a]}'\n",
    "                for x in self.P[s][a]:\n",
    "                    string += f'\\n| probability:{x[0]}, '\n",
    "                    string += f'new_state:{np.unravel_index(x[1], self.shape)}, '\n",
    "                    string += f'reward:{x[2]}, '\n",
    "                    string += f'done?:{x[3]} |'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos algunas características de la clase `GridworldEnv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acciones posibles: [0, 1, 2, 3]\n",
      "\n",
      "Número de acciones posibles: 4\n",
      "\n",
      "Una acción posible seleccionada al azar: ➡\n",
      "\n",
      "El estado actual es: 2\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv()\n",
    "print(\"Acciones posibles:\", env.action_space)\n",
    "print('')\n",
    "print(\"Número de acciones posibles:\", len(env.action_space))\n",
    "print('')\n",
    "a = random.choice(env.action_space)\n",
    "print(\"Una acción posible seleccionada al azar:\", dict_acciones[a])\n",
    "print('')\n",
    "print(\"El estado actual es:\", env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al imprimir el objeto, podemos ver el modelo del MDP que ha sido implementado mediante el método `_transition_prob`, el cual define, para cada estado y acción, la siguiente tupla: \n",
    "\n",
    "(probabilidad, próximo estado, recompensa, finalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "State:(0, 0)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 0), reward:0, done?:True |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(0, 0), reward:0, done?:True |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(0, 0), reward:0, done?:True |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(0, 0), reward:0, done?:True |\n",
      "--------------------\n",
      "State:(0, 1)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 1), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(0, 2), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(1, 1), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(0, 0), reward:-1, done?:True |\n",
      "--------------------\n",
      "State:(0, 2)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 2), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(0, 3), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(1, 2), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(0, 1), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(0, 3)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 3), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(0, 3), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(1, 3), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(0, 2), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(1, 0)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 0), reward:-1, done?:True |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(1, 1), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(2, 0), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(1, 0), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(1, 1)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 1), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(1, 2), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(2, 1), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(1, 0), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(1, 2)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 2), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(1, 3), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(2, 2), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(1, 1), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(1, 3)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(0, 3), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(1, 3), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(2, 3), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(1, 2), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(2, 0)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(1, 0), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(2, 1), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 0), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(2, 0), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(2, 1)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(1, 1), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(2, 2), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 1), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(2, 0), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(2, 2)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(1, 2), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(2, 3), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 2), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(2, 1), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(2, 3)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(1, 3), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(2, 3), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 3), reward:-1, done?:True |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(2, 2), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(3, 0)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(2, 0), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(3, 1), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 0), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(3, 0), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(3, 1)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(2, 1), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(3, 2), reward:-1, done?:False |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 1), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(3, 0), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(3, 2)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(2, 2), reward:-1, done?:False |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(3, 3), reward:-1, done?:True |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 2), reward:-1, done?:False |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(3, 1), reward:-1, done?:False |\n",
      "--------------------\n",
      "State:(3, 3)\n",
      "Action:⬆\n",
      "| probability:1.0, new_state:(3, 3), reward:0, done?:True |\n",
      "Action:➡\n",
      "| probability:1.0, new_state:(3, 3), reward:0, done?:True |\n",
      "Action:⬇\n",
      "| probability:1.0, new_state:(3, 3), reward:0, done?:True |\n",
      "Action:⬅\n",
      "| probability:1.0, new_state:(3, 3), reward:0, done?:True |\n"
     ]
    }
   ],
   "source": [
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los métodos más importantes de la clase es el `step()`, el cual recibe una acción como argumento y, junto con la información del estado actual y el modelo de transiciones, obtiene el estado al que pasa el sistema y devuelve una recompensa. También se obtiene un valor booleano que indica si el estado obtenido es terminal o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado=6, Recompensa=-1, Finalizado=False\n",
      "o___\n",
      "__x_\n",
      "____\n",
      "___o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv()\n",
    "obs, reward, done = env.step(SOUTH)\n",
    "print(f'Estado={obs}, Recompensa={reward}, Finalizado={done}')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Ejercicio 1:** \n",
    "\n",
    "([Próximo ejercicio](#ej2))\n",
    "\n",
    "Cree una pequeña función para hacer una caminata aleatoria por la rejilla hasta que el agente llegue a un estado terminal. Encuentre la utlidad del episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@___\n",
      "____\n",
      "____\n",
      "___o\n",
      "\n",
      "-6\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv()\n",
    "env.render()\n",
    "done = False\n",
    "utilidad = 0\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = random.choice(env.action_space)\n",
    "    obs, reward, done = env.step(action)\n",
    "    utilidad += reward\n",
    "    clear_output(wait = True)\n",
    "    env.render()\n",
    "    sleep(.10)\n",
    "\n",
    "print(utilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tienda <a class=\"anchor\" id=\"tienda\"></a>\n",
    "\n",
    "([Volver a Ejemplos](#impl))\n",
    "\n",
    "Recordemos el ejemplo discutido en las diapositivas. Imagine que estamos a cargo de una tienda que solo vende un artículo. Es necesario tener existencias del mismo para poder venderlas, pero como el local lo cobran por metro cuadrado, no queremos tener demasiadas existencias. \n",
    "\n",
    "<img src=\"./imagenes/tienda.png\" width=\"200\"/>\n",
    "\n",
    "La implementación inicial es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the actions\n",
    "NONE = 0\n",
    "RESTOCK = 1\n",
    "dict_acciones = {0:'None', 1:'RESTOCK'}\n",
    "\n",
    "class TiendaEnv():\n",
    "\n",
    "    def __init__(self, p_sale=0.2):\n",
    "        self.nS = 3 # number of states\n",
    "        self.nA = 2 # number of actions\n",
    "        self.p_sale = p_sale # probability of selling\n",
    "        self.state = 2\n",
    "\n",
    "        P = {} # initialize model (dict of dicts)\n",
    "        for s in range(self.nS):\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            # Per state and action provide list as follows\n",
    "            # P[state][action] = [(probability, next_state, reward, done)]\n",
    "            # Assignment is obtained by means of method _transition_prob\n",
    "            P[s][NONE] = self._transition_prob(s, NONE)\n",
    "            if s != 2:\n",
    "                P[s][RESTOCK] = self._transition_prob(s, RESTOCK)\n",
    "        self.P = P\n",
    "        \n",
    "    def _transition_prob(self, state, action):\n",
    "        if state == 0 and action == NONE:\n",
    "            return [(1, 0, 0, False)]\n",
    "        else:\n",
    "            if action == NONE:\n",
    "                return [(self.p_sale, state-1, 1, False), (1-self.p_sale, state, 0, False)]\n",
    "            else:\n",
    "                return [(self.p_sale, state, 1, False), (1-self.p_sale, state+1, 0, False)]            \n",
    "       \n",
    "    def render(self):\n",
    "        fig, axes = plt.subplots(figsize=(3, 1))\n",
    "        imagen = \"./imagenes/caja.png\"\n",
    "        arr_img = plt.imread(imagen, format='png')\n",
    "        imagen = OffsetImage(arr_img, zoom=0.5)\n",
    "        imagen.image.axes = axes\n",
    "        paso = 1./3\n",
    "        offsetX = 0\n",
    "        offsetY = 0\n",
    "        Y = 0\n",
    "        for X in range(0, self.state):\n",
    "            ab = AnnotationBbox(\n",
    "                    imagen,\n",
    "                    [(X*paso) + offsetX, (Y*paso) + offsetY],\n",
    "                    frameon=False)\n",
    "            axes.add_artist(ab)\n",
    "        axes.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for s in range(self.nS):\n",
    "            string += '\\n'+'-'*20\n",
    "            string += f'\\nState:{s}'\n",
    "            for a in range(self.nA):\n",
    "                string += f'\\nAction:{dict_acciones[a]} | {self.P[s][a]} |'\n",
    "        return string\n",
    "    \n",
    "    def step(self, action):\n",
    "        pass\n",
    "        # AQUÍ COMIENZA SU CÓDIGO\n",
    "        \n",
    "        s = self.state\n",
    "        p = self.P[s][action]\n",
    "        \n",
    "        indice = random.choices(\n",
    "            population = range(len(p)),\n",
    "            weights = [x[0] for x in p],\n",
    "            k = 1)[0]\n",
    "        \n",
    "        new_state = p[indice][1]\n",
    "        self.state = new_state\n",
    "        reward = p[indice][2]\n",
    "        done = p[indice][3]\n",
    "        return new_state, reward, done \n",
    "        \n",
    "        # AQUÍ TERMINA SU CÓDIGO\n",
    "        \n",
    "    def get_valid_actions():\n",
    "        pass\n",
    "        # AQUÍ COMIENZA SU CÓDIGO\n",
    "        \n",
    "        # AQUÍ TERMINA SU CÓDIGO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = TiendaEnv(p_sale=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el modelo del MDP que ha sido implementado mediante el método `_transition_prob`, el cual define, para cada estado y acción, la tupla \n",
    "\n",
    "(probabilidad, próximo estado, recompensa, finalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "State:0\n",
      "Action:None | [(1, 0, 0, False)] |\n",
      "Action:RESTOCK | [(0.4, 0, 1, False), (0.6, 1, 0, False)] |\n",
      "--------------------\n",
      "State:1\n",
      "Action:None | [(0.4, 0, 1, False), (0.6, 1, 0, False)] |\n",
      "Action:RESTOCK | [(0.4, 1, 1, False), (0.6, 2, 0, False)] |\n",
      "--------------------\n",
      "State:2\n",
      "Action:None | [(0.4, 1, 1, False), (0.6, 2, 0, False)] |\n",
      "Action:RESTOCK | [] |\n"
     ]
    }
   ],
   "source": [
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, vemos que con el estado 0 y la acción 0 (NONE) se tiene\n",
    "\n",
    "(1, 0, 0, False)\n",
    "\n",
    "Esto quiere decir que con probabilidad 1 el estado que se obtiene es el estado 0, con recompensa 0. Además, el estado 0 no es estado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Ejercicio 2:** \n",
    "\n",
    "([Anterior ejercicio](#ej1)) ([Próximo ejercicio](#ej3))\n",
    "\n",
    "Uno de los métodos más importantes que no está implementado es el de `step()`, el cual recibe una acción como argumento y, junto con la información del estado actual y el modelo de transiciones, obtiene una tripla `(state, reward, done)`, que consiste del estado al que pasa el sistema, la recompensa obtenida, y un valor booleano que indica si el estado obtenido es terminal o no. Implemente dicho método en la clase `TiendaEnv`. \n",
    "\n",
    "**Nota:** Observe que el modelo está guardado en el atributo `self.P` de la forma \n",
    "\n",
    "P[state][action] = [(probability, next_state, reward, done)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado inicial: 2\n",
      "Estado obtenido con la acción 0: 2\n",
      "Recompensa obtenida con la acción 0: 0\n",
      "Finalizado? False\n"
     ]
    }
   ],
   "source": [
    "#def step(self, action):\n",
    "#    pass\n",
    "    # AQUÍ SU CÓDIGO \n",
    "    \n",
    "    # AQUÍ TERMINA SU CÓDIGO\n",
    "\n",
    "#setattr(TiendaEnv, \"step\", step)\n",
    "\n",
    "env = TiendaEnv(p_sale=0.4)\n",
    "print('Estado inicial:', env.state)\n",
    "a = 0\n",
    "obs, reward, done = env.step(a)\n",
    "print(f'Estado obtenido con la acción {a}: {obs}')\n",
    "print(f'Recompensa obtenida con la acción {a}: {reward}')\n",
    "print('Finalizado?', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Ejercicio 3:** \n",
    "\n",
    "([Anterior ejercicio](#ej2)) ([Próximo ejercicio](#ej4))\n",
    "\n",
    "\n",
    "Otro método importante que no está implementado es el de determinar cuáles acciones son posibles en un estado dado, llamémoslo `get_valid_actions()`. Implemente dicho método en la clase `TiendaEnv`. \n",
    "\n",
    "Observe que la acción NONE es posible en todos los estados, pero la acción RESTOCK no es posible en el estado 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de políticas <a class=\"anchor\" id=\"poli-eval\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a usar la programación dinámica para encontrar los valores de estado para una política dada. La idea central es usar la ecuación de Bellman como una regla iterativa:\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_{s'}\\left( p(s' | s,\\pi(s)) \\Bigl[ r + \\gamma v_k(s') \\Bigr] \\right)$$\n",
    "\n",
    "Esto da lugar al siguiente algoritmo:\n",
    "\n",
    "<img src=\"./imagenes/policy_evaluation.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Ejercicio 4:** \n",
    "\n",
    "([Anterior ejercicio](#ej3)) ([Próximo ejercicio](#ej5))\n",
    "\n",
    "Implemente el algoritmo iterativo de evaluación de políticas y utilícelo para encontrar los valores de la política `policy` definida en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acciones = {0:\"⬆\", 1:\"➡\", 2:\"⬇\", 3:\"⬅\"}\n",
    "policy = [NORTH, EAST, EAST, SOUTH] * 4\n",
    "pp = np.reshape(policy, (4,4))\n",
    "print(np.vectorize(dict_acciones.get)(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy evaluation\n",
    "\n",
    "def policy_eval(env, policy, discount_factor=1.0, theta=0.01, verbose=0):\n",
    "    \"\"\"\n",
    "    Evalúa una política para un entorno.\n",
    "    Input:\n",
    "        - env: transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - policy: vector de longitud env.nS que representa la política\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: 0 no imprime nada, \n",
    "                   1 imprime la iteración del valor\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    # AQUÍ SU CÓDIGO\n",
    "    \n",
    "    # AQUÍ TERMINA SU CÓDIGO\n",
    "\n",
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "policy = [NORTH, EAST, EAST, SOUTH] * 4\n",
    "p = policy_eval(env, policy, discount_factor=1, theta=0.1, verbose=0)\n",
    "pp = np.reshape(p, shape)\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** El resultado de debe ser \n",
    "```\n",
    "[[ 0. -5. -4. -3.]\n",
    " [-1. -4. -3. -2.]\n",
    " [-2. -3. -2. -1.]\n",
    " [-3. -2. -1.  0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejoramiento de políticas <a class=\"anchor\" id=\"dp\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Recordemos que el propósito del RL es encontrar la acción que mejor utilidad tenga en cada estado, determinando así la política óptima para el problema. Si conocemos el modelo de un MDP, podemos ir mejorando una política paso a paso. Los dos métodos de esta sección realizan el mejoramiento de una política hasta llegar a la política óptima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration <a class=\"anchor\" id=\"poli-iter\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "En este algoritmo se busca mejorar una política $\\pi$ en cada estado $s$, definiendo $\\pi'$ de tal manera que:\n",
    "\n",
    "$$\\pi'(s) = \\mbox{arg}\\max_a q_{\\pi}(s,a)$$\n",
    "\n",
    "Esto da lugar a una nueva política $\\pi'$. Luego, se recalculan los valores $v_{\\pi'}(s)$ usando el algoritmo de evaluación de política visto en la sección anterior. Este proceso se itera hasta converger a la política óptima:\n",
    "\n",
    "<img src=\"./imagenes/p_i.png\" width=\"350\"/>\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/policy_iteration1.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej5\"></a>**Ejercicio 5:** \n",
    "\n",
    "([Anterior ejercicio](#ej4)) ([Próximo ejercicio](#ej6))\n",
    "\n",
    "Implemente el algoritmo de policy improvement y mejore la política del ejercicio 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement\n",
    "\n",
    "def policy_iteration(env, pol, discount_factor=1.0, theta=0.01, verbose=0):      \n",
    "    \"\"\"\n",
    "    Mejoramiento de una política.\n",
    "    Input:\n",
    "        - env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - pol: vector de longitud env.nS que representa la política\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: 0 no imprime nada, \n",
    "                   1 imprime la iteración de la política,\n",
    "                   2 imprime también la iteración del valor\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la política óptima.\n",
    "    \"\"\"    \n",
    "    pass\n",
    "    # AQUÍ SU CÓDIGO\n",
    "    \n",
    "    # AQUÍ TERMINA SU CÓDIGO\n",
    "\n",
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "policy = [NORTH, EAST, EAST, SOUTH] * 4\n",
    "pp = np.reshape(policy, shape)\n",
    "print('Política inicial:')\n",
    "print(np.vectorize(dict_acciones.get)(pp))\n",
    "print('')\n",
    "p = policy_iteration(env, policy, discount_factor=1, theta=0.01, verbose=0)\n",
    "pp = np.reshape(p, shape)\n",
    "print('Política óptima:')\n",
    "print(np.vectorize(dict_acciones.get)(pp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** La respuesta debe ser:\n",
    "\n",
    "```\n",
    "Política óptima:\n",
    "[['⬆' '⬅' '⬅' '⬇']\n",
    " ['⬆' '⬆' '⬆' '⬇']\n",
    " ['⬆' '⬆' '➡' '⬇']\n",
    " ['⬆' '➡' '➡' '⬆']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration <a class=\"anchor\" id=\"value-iter\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Para mejorar el desempeño del algoritmo de policy iteration, se puede truncar la evaluación de la política después de una iteración para cada estado. Además, se puede combinar, en una sola regla iterativa, el mejoramiento de la política con la evaluación truncada de la política:\n",
    "\n",
    "$$v_{k+1}(s) = \\max_{a}\\sum_{s'}\\left( p(s' | s, a) \\Bigl[ r + \\gamma  v_{k}(s') \\Bigr] \\right)$$\n",
    "\n",
    "Se puede demostrar que la sucesión $\\{v_k\\}$ converge a $v_*$. \n",
    "\n",
    "Finalmente, para obtener la política óptima $\\pi_*$ se buscan las acciones mediante el argmax de los valores óptimos obtenidos en el proceso anterior:\n",
    "\n",
    "$$\\pi_*(s) = \\mbox{arg}\\max_a\\sum_{s'}\\left( p(s' | s, a) \\Bigl[ r + \\gamma  v_{*}(s') \\Bigr] \\right)$$\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/value_iteration.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej6\"></a>**Ejercicio 6:** \n",
    "\n",
    "([Anterior ejercicio](#ej5))\n",
    "\n",
    "Implemente el algoritmo de value-iteration para encontrar la política óptima del MDP. Use su algoritmo para encontrar la política óptima del Grid World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration\n",
    "\n",
    "def value_iteration(env, discount_factor=1.0, theta=0.01, verbose=0):\n",
    "    \"\"\"\n",
    "    Mejoramiento de una política.\n",
    "    Input:\n",
    "        - env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: 0 no imprime nada, \n",
    "                   1 imprime la iteración de la política,\n",
    "                   2 imprime también la iteración del valor\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la política óptima.\n",
    "    \"\"\" \n",
    "    pass\n",
    "    # AQUÍ SU CÓDIGO\n",
    "    \n",
    "    # AQUÍ TERMINA SU CÓDIGO\n",
    "\n",
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "p = value_iteration(env, discount_factor=1, theta=0.01, verbose=0)\n",
    "pp = np.reshape(p, shape)\n",
    "print('Política óptima:')\n",
    "print(np.vectorize(dict_acciones.get)(pp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de tiempos <a class=\"anchor\" id=\"comp\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Vamos a hacer el estudio empírico de la complejidad de tiempos de los dos algoritmos. Correremos ambos algoritmos sobre el Grid World con tamaños (4,4) hasta (9,9). Con cada ambiente correremos 10 cada algoritmo y registraremos los tiempos de máquina usados para encontrar la política óptima. Los resultados son los siguientes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = lambda env: policy_iteration(env, get_nice_policy(env.shape))\n",
    "v_i = lambda env: value_iteration(env)\n",
    "funs = [p_i, v_i]\n",
    "nombres_funs = ['policy-iteration', 'value-iteration']\n",
    "shapes = [(n,n) for n in range(4,15)]\n",
    "lista_args = [GridworldEnv(shape) for shape in shapes]\n",
    "df = compara_entradas_funs(funs, nombres_funs, lista_args, N=10)\n",
    "sns.lineplot(x='Long_entrada',y='Tiempo',hue='Funcion',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gráfica muestra el tiempo promedio que cada algoritmo toma para encontrar la política óptima con distintos tamaños del Grid World. A partir de la gráfica queda muy claro que el algoritmo de `value-iteration` es más eficiente que el de `policy-iteration`. Observe también que el primero no necesita una política de entrada, mientras que sí se requiere una política como argumento del `policy-iteration`. En este ejemplo se tomó una política que converge relativamente rápido, pero otras políticas toman muchísimo más tiempo en converger. Todo esto muestra las ventajas del `value-iteration`, el cual es más rápido y no requiere política de entrada.\n",
    "\n",
    "Adicionalmente, observe que la complejidad de tiempo va creciendo mucho en ambos casos, lo cual hace que sean inviables para MDPs que tienen una gran cantidad de estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En este notebook usted aprendió\n",
    "\n",
    "* Cómo implementar MDP en python usando la librería `gym` de OpenAI.\n",
    "* Cómo implementar la evaluación de una política, para obtener los valores en cada estado.\n",
    "* Cómo implementar la metodología de mejoramiento de políticas mediante policy iteration y value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Shanghi, N. (2021) Deep Reinforcement Learning with Python: With PyTorch, TensorFlow and OpenAI Gym. Apress. \n",
    "\n",
    "Sutton R., & Barto, A., (2015) Reinforcement Learning: An Introduction, 2nd Edition. A Bradford Book. Series: Adaptive Computation and Machine Learning series. \n",
    "\n",
    "Winder, P., (2021) Reinforcement Learning: Industrial Applications of Intelligent Agents. O’Relly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
